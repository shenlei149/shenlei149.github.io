## The Notion of Probability
扔一个硬币四次，想知道至少三次正面（头）朝上的概率。结果一共有$2^4=16$种，关心的结果的数量是五种，分别是$HHHH,HHHT,HHTH,HTHH,THHH$。那么至少三次正面向上的概率就是关心的结果个数比上总的结果的个数，$5/16$。这个形成概率概念的常识性做法，当然，稍微有点不严谨，比如没有说硬币是公平的，正面和反面向上的概率是相同的。  
**Definition 15.1.** 令$\Omega$是实验的某些序列的结果的有限集合，各个结果的发生是等概率的。令$A\subseteq \Omega$，那么$\Omega$称为样本空间(`sample space`)，$A$是事件(`event`)，那么比率
$$P(A)=\frac{|A|}{|\Omega |}$$
是事件$A$的概率。  
$P$是定义在$\Omega$的所有子集的结合上的函数，取值范围是$0\leq P(A)\leq 1$。  
当$A,\Omega$是无限集合的时候，这个定义是无意义的。这里我们只讨论有限集合的概率问题。  
如果$A,B$是$\Omega$的不相交子集，并且$|A\cup B|=|A|+|B|$，因此$P(A\cup B)=P(A)+P(B)$。一般地，$|A\cup B|=|A|+|B|-|A\cap B|$，蕴含$P(A\cup B)=P(A)+P(B)-P(A\cap B)$。进一步泛化可以得到下面这个简单但是有用的不等式。  
**Proposition 15.2.** 令$A_1,A_2,\cdots,A_n$来自同一个样本空间，那么
$$P(A_1\cup A_2\cup\cdots\cup A_n)\leq P(A_1)+P(A_2)+\cdots +P(A_n)$$
**Proof.** 上式等价于
$$|A_1\cup A_2\cup\cdots\cup A_n|\leq |A_1|+|A_2|+\cdots +|A_n|$$
左边对于至少属于其中一个$A_i$的样本空间的元素只计数了一次，而右边至少计数了一次。

三至七章有大量排列组合的例子和练习，它们能很容易的转化成概率的例子和练习。比如有多少个六位数包含数字6，可以变成随机取一个六位数字，包含6的概率是多少。所以这里就不再讨论这些基础问题。这一小节给出两个反直觉的例子。

**Example 15.3.** 彩票游戏。从36个数字中选择6个，至少选中一个中奖数字的概率是多少？  
**Solution.** 令$A$表示至少选中一个中奖数字，$B$表示不包含中奖数字。很明显，$A, B$是不相交的，且$A+B=\Omega$，所以$P(A)+P(B)=1$。因此我们计算$P(B)$的概率即可，$B$事件是从30个数字选择6个。
$$P(A)=1-P(B)=1-\frac{\begin{pmatrix}
30\\6
\end{pmatrix}}{\begin{pmatrix}
36\\6
\end{pmatrix}}=1-0.3048=0.6952$$
所以任意选择六个数字，至少有一个中奖数字的概率接近百分之七十。

如果$A, B$是不相交的，我们称之为互斥(`mutually exclusive`)。也就是说，$A, B$不可能同时发生。在此基础上，如果$A+B=\Omega$，那么$B$是$A$的补集(`complement`)，记作$\bar{A}=B$。

**Example 15.4.** 某个聚会上有四十个人，没有人是二月二十九号出生的。Adam向Bill提意玩一个游戏，每一个客人写下出生的月份和天，如果两个人是同一天则Adam赢，否则Bill赢。Bill看了看，只有四十个人，比一年的天数小多了，觉得自己会赢。实际情况呢？
**Solution.** Bill没有分清百分百会发生和大于百分之五十会发生。如果一定赢，那么需要366个人，但是超过一半的机会是完全不同的问题。  
下面会证明至少需要23个人，Adam大概率会赢，否则Bill会赢。  
至少有两人在同一天生日的补集是没有两个人的生日在同一天。第一个人可以是任意一天出生，365个可能性，第二个人不能是同一天，364个可能性，依此类推。23个人可能总数是$(365)_{23}$。所有可能的结果集有$365^{23}$。所有没有任何两个人的生日在同一天的概率是
$$\frac{365\cdot 364\cdots 343}{365^{23}}<\frac{1}{2}$$
所以超过一半的可能性是存在两个人的生日在同一天。  w
我们为什么要排除二月二十九号呢？因为这一天和其他天不等概率，闰年才会有这一天，几率是其他天的四分之一（近似）。那么为了使得样本空间是等概率的，需要考虑四年，$4\cdot 365+1=1461$天作为整个样本空间，使得论证过程会复杂很多。  
上面这个例子有时被称为生日悖论(`birthday paradox`)。

## Non-constructive Proofs
如果一个盒子里面装者一些球，随机挑一个是蓝球的概率大于0，那么至少有一个蓝球。这个看似简单实则很有用的。  
第十三章提到了`Ramsey number`$R(k,k)$，如果我们用两种颜色对$R(k,k)$个点的完全图染色，总能找到一个完全子图$K_k$是同色的。  
下面要证明下界$R(k,k)>2^{k/2}$。这个命题是说对$2^{k/2}$个顶点的完全图染色，可能不存在一种染色能够找到一个同色的完全子图$K_k$。第十三章的证明了$R(3,3)>5, R(5,5)>17$，明确找出了不符合题意的图。但是回到这个问题上，不需要这么严格。只需要证明上述所阐述的存在性即可，而不需要找到这样一种染色。  
**Theorem 15.5.** 对所有正整数$k\geq 3$，不等式$R(k,k)>2^{k/2}$都成立。  
**Proof.** 令$G=K_n$，对$G$按照如下方式染色：对于每条边而言，通过掷硬币的方式，如果正面向上染成红色，否则染成蓝色，每条边是红色或者蓝色的概率都是二分之一。我们将会证明没有同色完全子图$K_k$的概率是大于0的。$p=\frac{|F|}{|\Omega |}$，$\Omega$是$n$个顶点的完全图进行二染色的集合，$p>0$意味着至少有一个二染色的完全图$K_n$不包含同色完全子图$K_k$。  
我们会通过证明$1-p<1$来证明$p>0$。前者的意思是对$G$随机染色，至少有一个同色完全子图的概率。  
将$K_n$中的$K_k$子图二颜色的方式有$2^{\begin{pmatrix}k\\2\end{pmatrix}}$，其中有两个是同色的。所以随机对其染色是同色的概率是
$$\frac{2}{2^{\begin{pmatrix}k\\2\end{pmatrix}}}=2^{1-{\begin{pmatrix}k\\2\end{pmatrix}}}$$
$K_n$有$\begin{pmatrix}n\\k\end{pmatrix}$个$K_k$。每一个都有可能包含同色的子图。根据**Proposition 15.2**这些子图中至少有一个同色子图的概率最多是这些可能性之和。如果用$A_S$表示$G$的$K_k$子图$S$是同色的，那么
$$P(\bigcup_S A_S)\leq \sum_S P(A_S)=\begin{pmatrix}
n\\k
\end{pmatrix}2^{1-{\begin{pmatrix}k\\2\end{pmatrix}}}\tag{15.1}$$
代入$n\leq 2^{k/2},k\geq 3$
$$\begin{pmatrix}
n\\k
\end{pmatrix}2^{1-{\begin{pmatrix}k\\2\end{pmatrix}}}<\frac{n^k}{k!}2^{1-{\begin{pmatrix}k\\2\end{pmatrix}}}\leq\frac{2\cdot 2^{k^2/2}}{k!2^{\begin{pmatrix}
k\\2
\end{pmatrix}}}=2\cdot\frac{2^{k/2}}{k!}<1$$
最后一步使用递归法很容易证明。

第十三章证明了$R(k,k)\leq 4^k$，这里证明了$\sqrt{2}^k<R(k,k)$。基本上这是已知的`Ramsey numbers`最好的范围了。

**Theorem 15.6.** 令$m,n$是大于1的正整数，并且$m\geq 2\log_2 n$。对$K_{n,n}$进二染色，没有同色边的$K_{m,m}$是可能的。  
**Proof.** 对$K_{m,m}$进行二颜色的方式有$2^{m^2}$种，其中两种是单色的。那么至少有一个同色子图的概率最多是$\begin{pmatrix}n\\m\end{pmatrix}^2 2^{1-m^2}$，我们需要证明
$$\begin{pmatrix}n\\m\end{pmatrix}^2 2^{1-m^2}$$
也就是
$$2\begin{pmatrix}n\\m\end{pmatrix}^2<2^{m^2}$$
插入两个中间表达式
$$2\begin{pmatrix}n\\m\end{pmatrix}^2<n^{2m}\leq (2^{m/2})^{2m}=2^{m^2}$$
中间的不等式用到的就是题目中$m,n$的关系。

可以从另一个角度来解读这个问题。如果$m\geq 2\log_2 n$，那么存在一个元素是零或一的$n\times n$的矩阵，没有$m\times m$的子矩阵只包含零或者只包含一。  
事实上，前面的证明只是存在性，没有找到这么一个染色，对应到这里，我们不知道怎么构造这个矩阵来满足要求。换句话说，知道是可能的和知道怎么到之间有巨大的鸿沟。目前已知的当$m=c\sqrt{n}$时来构造满足要求的矩阵。但这距离我们证明为真的命题$m=2\log_2 n$之间也就很大的距离。

## Independent Events
### The Notion of Independence and Bayes' Theorem
扔两个骰子。$A$表示第一个骰子是六这个事件，$B$表示第二个骰子是六这个事件，那么$P(A)=P(B)=1/6, P(A\cap B)=1/36$，可以看出来$P(A)\cdot P(B)=P(A\cap B)$，这是个巧合吗？从$[12]$中任选一个数，$C$表示该数能被2整除，$D$表示概述能被3整除，$F$表示该数能被4整除。那么$P(C)=1/2,P(D)=1/3,P(F)=1/4$，接着可以计算得到$P(C\cap D)=1/6,P(D\cap F)=1/12$，看似乘积关系依旧成立。但是$P(C\cap F)=P(F)=1/4\neq P(C)P(F)$。  
为什么有时$P(A)\cdot P(B)=P(A\cap B)$但是有时$P(A)\cdot P(B)\neq P(A\cap B)$？因为有的时候$A$的发生增加或者减少了$B$发生的概率，有的时候$A$对$B$没有影响。回到前面从12个数中选择一个的问题。能被2整除增加了能被4整除的概率。全集从12个减少为6个，但是能被4整除的结果集个数没有变化。相反，能被2整除对能否被3整除没有影响。全集从12个减少到了6个，同时，能被3整数的结果集的个数也有相应的减少（从4个到2个）。

**Definition 15.7.** 如果事件$A,B$来自同一样本空间$\Omega$，有$P(A)\cdot P(B)=P(A\cap B)$，那么$A,B$称为独立事件(`independent events`)，否则是非独立事件(`dependent`)。

**Definition 15.8.** 令事件$A,B$来自同一样本空间，且假设$P(B)>0$，有
$$P(A|B)=\frac{P(A\cap B)}{P(B)}$$
$P(A|B)$是条件概率(`conditional probability`)，表示给定事件$B$发生的前提下$A$发生的概率。

**Proposition 15.9.** 事件$A,B$是独立事件等价于$P(A|B)=P(A)$成立。  
换句话说，事件$A,B$是独立的等价于$B$的发生不会使得$A$更容易或更不容以发生。

**Example 15.10.** 连续扔四次硬币。看不最后的结果，只知道至少有两次正面向上，问四次都向上的概率是多少？  
**Solution.** 令$A$表示四次正面向上，$B$表示至少两次正面向上，那么$A\cap B=A$。所以$P(A|B)=P(A)/P(B)$。每次扔掷硬币正面向上的概率是二分之一，所以$P(A)=\frac{1}{16}$。有$1/16$的几率四次正面向上，$4/16$的几率三次正面向上，$6/16$的几率两次正面向上，所以$P(B)=\frac{11}{16}$。$P(A|B)=\frac{1}{11}$。

**Example 15.11.** 令$p=p_1p_2p_3\cdots p_n$是一个随机挑选的$n$排列。令$A$事件是$p_1>p_2$，$B$事件是$p_2>p_3$。计算$P(A|B)$，$A,B$是否是独立事件？  
**Solution.** 明显$P(A)=P(B)=1/2$。$A\cap B$是$p_1>p_2>p_3$，概率是六分之一。因此
$$P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{1/6}{1/2}=\frac{1}{3}\neq P(A)$$
所以$A,B$不是独立事件。

你的第一反应$A,B$明显不是独立事件，因为如果$p_1>p_2$成立的话，那么$p_2$较小，所以$p_2>p_3$成立的可能性会小一些。当处理条件概率时利用直觉的话一定要小心。比如下面这个例子就和直觉相悖。  
**Example 15.12.** 一个大学两个学院，人文学院和工程学院，两个学院统计上学年申请人的情况，都是国内学生的入学比例比国际学生的高。那么对于整个大学而言，国内学生入学比例比国际学生高吗？  
**Solution.** 答案是不能。比如下表就是个反例。
|  |Liberal Arts|Engineering|Entire University|
|--|--|--|--|
|Domestic<br>applicants|Admitted: 10<br>Applied:120<br>success rate:8.3%|Admitted: 10<br>Applied:10<br>success rate:100%|Admitted: 20<br>Applied:130<br>success rate:15.9%|
|International<br>applicants|Admitted: 1<br>Applied:15<br>success rate:6.7%|Admitted: 90<br>Applied:100<br>success rate:90%|Admitted: 91<br>Applied:115<br>success rate:79.1%|

这个反例是著名的辛普森悖论(`Simpson's paradox`)。对工程学院而言，虽然国内学生入学比例高，但是国际学生占比非常高，国内学生的申清者只占全校国内学生申清总数的8%，国际学生的申请者占全校国际学生申请人数的85%以上。而人文学院恰恰相反，并且国际学生的申清人数很少，不会对国际学生的入学比例有大的影响，而国内学生的申清总数很大，入学比例低，一下子就大大减少了国内学生申清成功的比例。  
下面的贝叶斯定理可以给出更精准的解释。  
**Theorem 15.13 (Bayes' Theorem).** 令$A,B$是互斥事件，那么$A\cup B=\Omega$。令$C$为任意事件，那么
$$P(C)=P(C|A)\cdot P(A)+P(C|B)\cdot P(B)$$
也就是说$C$的概率是条件概率的加权平均数。  
**Proof.** 由于$A,B$互斥，那么$A\cap C$和$B\cap C$不相交，又因为$A\cup B=\Omega$，它们的并集就是$C$。因此
$$P(C)=P(A\cap C)+P(B\cap C)$$
上式右边的第一项（第二项）和要证明的等式右边的第一项（第二项）是相等的，就是条件概率的公式。

现在我们可以对前面的辛普森悖论有更深的理解了。令$A_1$表示事件——国际学生申请人文学院，$B_1$表示国内学生申请人文学院，类似的$A_2,B_2$表示申请工程学院，$C_1,C_2$表示国际学生、国内学生申请整个大学。根据**Theorem 15.13**
$$P(C_1)=P(C_1|A_1)\cdot P(A_1)+P(C_1|B_1)\cdot P(B_1)$$
$$P(C_2)=P(C_2|A_2)\cdot P(A_2)+P(C_2|B_2)\cdot P(B_2)$$
要求国内学生有更大的机会被任何一学院录取的标准确保了$P(C_1|A_1)<P(C_2|A_2),P(C_1|B_1)<P(C_2|B_2)$。但是对任意$P(A_1),P(B_1)$（$A_2$是$A_1$的补集，$B_2$是$B_1$的补集）而言，命题能不成立呢？我们可以选择$A_1,B_1$对$P(C_1)$非常有利且对$P(C_2)$非常不利。怎么选择呢？如果$P(C_1|A_1)$很大，选择一个很大的$P(A_1)$，反之$P(C_1|A_1)$很小的话，可以选择一个很小的$P(A_1)$；选择$P(B_1)$也类似。  
换句话说，加权平均值比非加权平均值难控制的多。事实上，如果要求$P(A_1)=P(B_1)=1/2$，甚至只要求$P(A_1)=P(B_1)$，那么对于整个大学而言，国内学生的入学比更高。

**Example 15.14.** 假设有两个人$A$和$B$，其中一人是罪犯。现在知道犯罪分子是特殊的$T$型血，人群中有百分之五的人是$T$型血。如果$A$是这种血型，那么是犯罪分子的概率是多少？  
**Proof.** 令$A,B$表示对应犯罪嫌疑人$A,B$是犯罪分子。$X$表示$A$是$T$型血。那么
$$P(A|X)=\frac{P(A\cap X)}{P(X)}=\frac{P(X|A)P(A)}{P(X)}$$
获得血型信息之前，$P(A)=P(B)=1/2$。如果$A$是犯罪分子，那么肯定是$T$型血，所以$P(X|A)=1$。如果$B$是犯罪分子，$A$是$T$型的概率是1/20，所以$P(X|B)=1/20$。根据**Bayes' Theorem**
$$P(X)=P(X|A)P(A)+P(X|B)P(B)=1\cdot\frac{1}{2}+\frac{1}{20}\cdot\frac{1}{2}=\frac{21}{40}$$
所以
$$P(A|X)=\frac{1\cdot\frac{1}{2}}{\frac{21}{40}}=\frac{20}{21}$$
所以如果附加了血型信息，那么$A$是犯罪分子的概率是20/21。

上面的例题中的方法非常有用。我们需要计算$P(A|X)$，直接计算不容易，但是反转条件，$P(X|A)$是容易确定的，那么我们可以利用$P(A\cap X)$等于$P(X|A)P(A)$，也等于$P(A|X)P(X)$。  
习题34也是类似的，是**Bayes' Theorem**的典型应用场景，解决一个乍看不容易的问题，结论是很重要的，而且反直觉。

### More Than Two Events
定义三个或多个事件相互独立不是显而易见的事情。可以类似的给出一个需要满足的等式$P(A_1\cap A_2\cdots\cap A_n)=P(A_1)P(A_2)\cdots P(A_n)$。如果$P(A_i)=0$，不管其他事件多么的相互依赖，等式都成立。为了有一些局部信息，可以强加要求$P(A_i\cap A_j)=P(A_i)P(A_j),i\neq j$，但是考虑下面的情况。
$A$表示从$[10]$随机选出一个奇数，$B$表示从$[20]$随机选出一个奇数，$C$表示两次选出来的数差值是奇数。  
不难证明$P(A)=P(B)=P(C)=1/2$，并且$A,B,C$两两独立。但是$P(A\cap B\cap C)=0\neq P(A)P(B)P(C)=1/8$，因此，我们也不想称这些事件相互独立。  
通过一个更强的要求：事件的任意子集都相互独立来解决上述的问题。

**Definition 15.15.** 我们称$A_1,A_2,\cdots,A_n$是相互独立的，当对于任意非空子集$S=\{i_1,i_2,\cdots,i_k\}\subseteq [n]$都有
$$P(A_{i_1}\cap A_{i_2}\cdots\cap A_{i_k})=P(A_{i_1})P(A_{i_2})\cdots P(A_{i_k})$$

**Theorem 15.16 (Bayes' Theorem, General Version).** 假设$A_1,A_2,\cdots,A_n$是同一样本空间$\Omega$的事件，有$A_1\cup A_2 \cup\cdots\cup A_n=\Omega$，并且$A_i\cap A_j=\emptyset,i\neq j$。令$C\subset \Omega$是任意事件
$$P(C)=\sum_{i=1}^nP(C|A_i)P(A_i)$$
**Proof.** 证明类似于**Theorem 15.13**。

一个著名的使用泛化版本的例子是三门问题(`Monty Hall problem`)。给出三个门，竞猜者选择一个自认为有大奖的一个，其余两个门里面是羊。一旦竞猜者做出了选择，主持人（知道三个门背后的东西）选择一个是羊的门打开，然后问竞猜者是否要更换选择。  
一个直观的想法是只有两个门，一个是羊，一个是大奖，换不换都一样，都是二分之一的概率选中。  
这个想法是错的。没有任何信息的情况下选择正确的概率是1/3，错误的概率是2/3，排除一个错误答案之后，选择不变就还是1/3的几率，变相当于选择到对立面，2/3的概率赢得大奖。  
从数学角度严格求解。令$A_1,A_2,A_3$表示大奖在门$1,2,3$的后面。不妨设竞猜者选择的一号门，主持人打开了二号门（不妨令其为事件$B$）。我们需要计算的是$P(A_1|B)$，就是主持人打开了二号门不换选择能够得到大奖的几率。  
$$P(A_1|B)=\frac{P(A_1\cap B)}{P(B)}=\frac{P(B|A_1)P(A_1)}{P(B|A_1)P(A_1)+P(B|A_2)P(A_2)+P(B|A_3)P(A_3)}$$
显然$P(A_1)=P(A_2)=P(A_3)=1/3$。如果一号门是大奖，那么主持人可以选择二号门或者三号门，所以$P(B|A_1)=1/2$，如果二号门后面是大奖，主持人不能开二号门，所以$P(B|A_2)=0$，如果三号门是大奖，主持人不得不开二号门，因为一号门已经被竞猜者选择了，所以$P(B|A_3)=1$。代入上面的式子得到
$$P(A_1|B)=\frac{\frac{1}{2}\cdot\frac{1}{3}}{\frac{1}{2}\cdot\frac{1}{3}+0\cdot\frac{1}{3}+1\cdot\frac{1}{3}}=\frac{\frac{1}{6}}{\frac{1}{2}}=\frac{1}{3}$$
选择不变只有1/3的几率得到大奖，所以选择改变选择比较好。

## Expected Values
随机变量(`random variable`)是定义在样本空间$\Omega$上的函数(`function`)，范围是实数的一个集合。比如，$\Omega$是所有$n$个带标签的点组成的图的集合，我们可以通过令$X(G)$是$G$的边的数量来定义随机变量$X$，也可以通过令$Y(G)$是$G$的连通分量的个数来定义随机变量$Y$。  
在同一个样本空间上，我们可以定义随机变量的和与积，$(X+Y)(u)=X(u)+Y(u),(X\cdot Y)(u)=X(u)\cdot Y(u)$。  
或许最有用的随机变量参数是期望值(`expected value`)，也被称为期望(`expectation`)，平均值(`average value` or `mean`)。

**Definition 15.17.** 令$X:\Omega\to R$是随机变量那么集合$S=\{X(u)|u\in\Omega\}$是有限的，也就是说$X$只能取有限多的值。那么
$$E(x)=\sum_{i\in S}i\cdot P(X=i)$$
是期望值，或被称为$X$在$\Omega$上的期望。  
这里$P(X=i)$是事件$X(u)=i$的概率，即
$$P(X=i)=\frac{|\{u\in\Omega|X(u)=i\}|}{|\Omega|}$$
换句话说，$E(X)$是$X$所有取值的加权平均数，这里的权重是$X$取对应值的概率。  
这就蕴含了
$$E(X)=\frac{1}{|\Omega|}\sum_{u\in\Omega}X(u)\tag{15.5}$$
注意：一些随机变量可以定义在许多不同的样本空间上。比如之前的例子，图的边数，可以定义在所有$n$个顶点的图上，也可以是所有$n$个顶点的连通图上，甚至可以是所有至多$3n$个顶点的图上，等等。对于每个例子，$S=\{X(u)|u\in\Omega\}$是不同的，那么$X$的期望也是不同的。因此，如果有含糊的可能性，我们写作$E_\Omega(X)$以示区别，否则简写作$E(X)$。  
有时我们一句话同时说明$X,\Omega$，比如，令$X(G)$是随机选择一个$n$个顶点的连通图的边数。这里的$\Omega$是所有$n$个顶点的连通图，$X(G)$是任意一个图$G\in\Omega$的边数。  
当$S=\{X(u)|u\in\Omega\}$是无穷集合时，在某些情况下也能定义$X$的期望。如果$S$是可数无限集，只要无穷求和存在，就可以定义$E(X)=\sum_{i\in S}i\cdot P(X=i)$。如果$S$不可数，需要用积分替代求和。

**Definition 15.18.** 对所有的$s,t$，随机变量$X,Y$是独立的(`independent`)，那么下面等式成立。
$$P(X=s,Y=t)=P(X=s)P(Y=t)$$

### Linearity of Expectation
对于任意实数$c$，通过令$cX(u)=c(X(u)),u\in\Omega$来定义随机变量$cX$。在计数组合学中，下面这个看似普通的定理非常有用。

**Theorem 15.19.**  
(1) 令$\Omega$上有两个随机变量$X,Y$，那么
$$E(X+Y)=E(X)+E(Y)$$
(2) 令$X$是一个随机变量，$c$实数，那么
$$E(cX)=cE(X)$$
所以“取期望值”是线性操作。这个定理不要求$X,Y$是独立的。不管它们之间多么交错复杂，不管它们多么难以计算，$X+Y$的期望值都能通过这个简单的公式计算得到。这也是为什么这个定理是最用的期望相关的属性，同时可以用于很多问题。  
**Proof.**  
(1) 根据$(15.5)$，
$$\begin{aligned}
E(X+Y)&=\frac{1}{|\Omega|}\sum_{u\in\Omega}(X+Y)(u)\\
&=\frac{1}{|\Omega|}\sum_{u\in\Omega}X(u)+\frac{1}{|\Omega|}\sum_{u\in\Omega}Y(u)\\
&=E(X)+E(Y)
\end{aligned}$$
(2) 类似地，
$$\begin{aligned}
E(cX)&=\frac{1}{|\Omega|}\sum_{u\in\Omega}cX(u)\\
&=c\frac{1}{|\Omega|}\sum_{u\in\Omega}X(u)\\
&=cE(X)
\end{aligned}$$
这个证明依赖于公式$(15.5)$，要求$\Omega$的每个事件是等概率的。但是**Theorem 15.19**是不要依赖这个条件的。  
**Proof.**  
(1) 令随机变量$X$有正数概率的值有$x_1,x_2,\cdots,x_n$，$Y$有正数概率的值有$y_1,y_2,\cdots,y_m$，那么
$$\begin{aligned}
E(X+Y)&=\sum_{i=1}^n\sum_{j=1}^m (x_i+y_j)P(X=x_i,Y=y_j)\\
&=\sum_{i=1}^n\sum_{j=1}^m x_iP(X=x_i,Y=y_j)+\sum_{i=1}^n\sum_{j=1}^m y_jP(X=x_i,Y=y_j)\\
&=\sum_{i=1}^n x_iP(X=x_i)+\sum_{j=1}^m y_jP(Y=y_j)\\
&=E(X)+E(Y)
\end{aligned}$$
(2) 令$r\in\Omega$，根据定义$(cX)(r)=cX(r)$，$X$取值$x_1,x_2,\cdots,x_n$，那么$P(cX=cx_i)=P(X=x_i)$。因此
$$\begin{aligned}
E(cX)&=\sum_{i=1}^n cx_iP(cX=cx_i)\\
&=c\sum_{i=1}^n x_iP(X=x_i)\\
&=cE(X)
\end{aligned}$$
令$p=p_1p_2\cdots p_n$是$n$排列，如果$p_i<p_{i-1},p_i<p_{i+1},2\leq i\leq n-1$，即$p_i$小于两边的元素，那么称之为谷(`valley`)。  
**Theorem 15.20.** 令$n\geq 2$，随机选择一个长度为$n$的排列，平均而言，有$(n-2)/3$个谷。  
如果不使用**Theorem 15.19**，我们必须要对于每个$j$计算有$j$个谷的$n$排列数量$v(j)$（这一步是困难的），然后计算$\sum_j j\cdot\frac{v(j)}{n!}$。而应用**Theorem 15.19**的话，小菜一碟。  
**Proof.** 取$n-2$个随机变量$Y_2,Y_3,\cdots,Y_{n-1}$。对某个$n$排列$p$，如果$i$是谷的话，$Y_i(p)=1$，否则$Y_i(p)=0$。对于每一个$p_i$而言，有三分之一的几率是谷，即$p_i$是集合$\{p_{i-1},p_i,p_{i+1}\}$中最小的。所以
$$E(Y_i)=\frac{1}{3}\cdot 1+\frac{2}{3}\cdot 0=\frac{1}{3}$$
定义$Y=Y_2+Y_3+\cdots +Y_{n-1}$，那么$Y$表示$p$的谷的个数。
$$E(Y)=\sum_{i=2}^{n-1} E(Y_i)=(n-2)E(Y_1)=\frac{n-2}{3}$$
类似于随机变量$Y_i$，如果某个事件发生则指为1，否则为0，被称为指示随机变量(`indicator (random) variables`)。  
**Theorem 15.21.** 随机选择一个$n$排列的固定点的个数的期望值是1。  
**Proof.** 定义$n$个随机变量$X_1,X_2,\cdots,X_n$。对于一个$n$排列$p$，令$X_i(p)=1$如果$p_i=i$，即$p$在位置$i$处有一个固定点，否则$X_i(p)=0$。  
$p_i$是随机的从$[n]$选取一个值，所以有$1/n$的可能性它的值是$i$。因此
$$E(X_i)=\frac{1}{n}\cdot 1+\frac{n-1}{n}\cdot 0=\frac{1}{n}$$
定义$X=X_1+X_2+\cdots+X_n$，那么$X(p)$就表示$p$的固定点的个数。应用**Theorem 15.19**求期望
$$E(X)=\sum_{i=1}^nE(X_i)=n\cdot E(X_1)=n\cdot\frac{1}{n}=1$$

### Existence Proofs Using Expectation
平均数不会比集合中最大的数大，同理，加权平均值也不会比集合中的最大值大。  
**Theorem 15.22.** 令$X:\Omega\to R$是随机变量，集合$S=\{X(u)|u\in\Omega\}$是有限集合，$j$是$S$中最大的元素，那么
$$j\geq E(X)$$
**Proof.** 运用$E(X)$定义
$$\begin{aligned}
E(X)&=\sum_{i\in S}i\cdot P(X=i)\\
&\leq j\sum_{i\in S}P(X=i)\\
&=j
\end{aligned}$$
**Theorem 15.23.** 令$G$是简单图，$n$个顶点$[n]$，$m$条边。$G$包含一个超过$m/2$条边的二分图。  
**Proof.** 令$G$分成两个不相交的非空子集$A,B$。移除$A,B$内部的边，生成一个二分图$H$。通过这种方式，$\Omega$有$2^{n-1}-1$种不同的二分图。令$X(H)$表示$H$中的边数。  
边$i$的两个顶点一个在$A$中一个在$B$中，那么$X_i=1$，否则$X_i=0$。  
$P(X_i=1)$是多少呢？$i$的顶点分别属于$A,B$，那么剩余$n-2$个顶点可以随意放，共$2^{n-2}$。因此$P(X_i=1)=\frac{2^{n-2}}{2^{n-1}-1}, P(X_i=0)=\frac{2^{n-2}-1}{2^{n-1}-1}$.
$$\begin{aligned}
E(X_i)&=0\cdot P(X_i=0)+1\cdot P(X_i=1)\\
&=\frac{2^{n-2}}{2^{n-1}-1}\\
&>\frac{1}{2}
\end{aligned}$$
对所有的边重复这个事情，即$X=X_1+X_2+\cdots+X_m$。根据**Theorem 15.19**
$$E(X)=\sum_{i=1}^mE(X_i)=mE(X_1)>\frac{m}{2}$$
所有$G$的二分子图的边的个数的期望值大于$m/2$，那么必然包含一个边数大于$m/2$的二分子图。

下面是复杂理论(`complexity theory`)中一个很有名的问题———中间性问题(`Betweenness problem`)。  
**Example 15.24.** 给定有序三元组$L_i=(a_i,b_i,c_i)$的列表$L=(L_1,L_2,\cdots ,L_k)$，对于任意$i$，$a_i,b_i,c_i$都是不同的数。不同下标$i,j$对应的$L$中包含的数可以相同。  
令$p=p_1p_2\cdots p_n$是$n$排列。如果$p$中有元素$b_i$大小介于$a_i,c_i$之间，就说$p$满足$L_i$。这里和顺序无关，这三个元素在$p$中可以是$a_ib_ic_i$也可以是$c_ib_ia_i$。  
存在一个排列$p$满足任意给定的$L$中至少三分之一的$L_i$。  
**Solution.** 随机变量$Y_i$表示随机选取一个$n$排列是否满足$L_i$。那么$P(Y_i=1)=\frac{1}{3}$，即有三分之一的概率$b_i$在$a_i,c_i$之间，所以$E(Y_i)=\frac{1}{3}$。根据**Theorem 15.19**
$$E(Y)=\sum_{i=1}^k E(Y_i)=\frac{k}{3}$$
根据**Theorem 15.22**可以推出存在性。

### Conditional Expectation
计算变量期望的另一种方式是条件期望。条件期望$E(X|A)$是事件$A$发生时$X$的期望值。通过替换$E(X)$中的绝对概率为$A$发生时的条件概率，可以定义$E(X|A)$。
$$E(X|A)=\sum_i i\cdot P(X=i|A)$$

扩展**Theorem 15.16**得到下面的定理。  
**Theorem 15.25.** 令$X$是随机变量，$A_1,A_2,\cdots,A_n$是样本空间$\Omega$的一系列事件，有$A_1\cup A_2\cup\cdots\cup A_n=\Omega$，且$A_i\cap A_j=\empty,i\neq j$。那么
$$E(X)=\sum_{i=1}^nE(X|A_i)P(A_i)$$
**Proof.** 接着**Theorem 15.16**，类比$X=j$是那个定理中的$C$。两边同乘以$j$，求和就可以得到要证明的式子了。

**Example 15.26.** 掷一个骰子三次。第一次结果至少是四，那么结果是偶数的次数的期望是多少？  
**Solution.** 后面两次结果是偶数的次数的期望值是一，如果第一次是偶数，那么期望值是二，如果第一次是奇数，那么期望值是一。所以
$$E(X)=\sum_{i=1}^2 E(X|A_i)P(A_i)=\frac{2}{3}\cdot 2+\frac{1}{3}\cdot 1=\frac{5}{3}$$

上面的例子$P(A_i)$比较容易算，下面这个就不这么明显了。  
**Example 15.27.** 一个球队赢球的概率是四分之三。如果已知前四场至少胜利了三场，那么12场比赛胜利的场次期望值是多少？  
**Solutioin.** 根据已知信息，前四场要么胜了三场（$A_1$）要么胜了四场（$A_2$）。忽略条件，前四场胜至少三场称为事件$B$。
$$P(A_1)=4\cdot\frac{1}{4}(\frac{3}{4})^3=\frac{27}{64}$$
$$P(A_2)=(\frac{3}{4})^4=\frac{81}{256}$$
$$P(B)=P(A_1)+P(A_2)=\frac{189}{256}$$
$$P(A_1|B)=\frac{P(A_1\cap B)}{P(B)}=\frac{4}{7}$$
$$P(A_2|B)=\frac{P(A_2\cap B)}{P(B)}=\frac{3}{7}$$
这里$B$就是样本空间。我们将$P(A_i|B)$记作$P_B(A_i)$。  
后面八场比赛，胜利的场次期望值是6场。加上前面四场，胜利场次用变量$X$表示
$$E_B(X)=E_B(X|A_1)P_B(A_1)+E_B(X|A_2)P_B(A_2)=9\cdot\frac{4}{7}+10\cdot\frac{3}{7}=9\frac{3}{7}$$
最终结果比9略大一些。因为我们知道了一些额外信息，前四场至少胜了三场。

## Exercises
(1) 令$p_n$是$n$个字符的字符串包含连续字符字串`Probability is fun`的概率。求证$\lim_{n\to\infty}p_n=1$。  
**Solution.** 首先，序列$\{p_n\}$是递增的，或者严格地说，是非递减的。$p_{n+1}=p_n+q_n$。$q_n$的含义是$n$个字符中没有包含需要的字符字串，但是$n+1$个包含的概率。  
接下来证明$\{p_n\}$会收敛到1。比如有序列$r_n=p_{16n}$，因为题目中的字符串有16个字符。  
令随机选择一个十六个字符的字符串是题目中要求的字符串是$a$事件，那么$a<1$，所以$r_n\geq 1-a^n$（可以把$16n$长度的字符串分割成$n$个十六个字符的字符串）。  
综上，有
$$1-a^n\leq r_n\leq 1$$
$n$趋于无穷时，$a^n$趋于0。

(13) 令$\pi$是随机的选择的$n$的分割。$X(p)$表示$\pi$的第一个部分的元素数量，$Y(p)$表示$\pi$的部分的个数。求$E(X)-E(Y)$。  
**Solution.** $\pi$的第一部分有$i$个元素，那么其共轭分割就有$i$个部分，那么$P(X=i)=P(Y=i)$，因此$E(X)-E(Y)=0$。

(16) 回忆十四章有$S_n(1234)<S_n(1324),n\geq 7$。令$n$是大于7的固定整数。令$A$事件表示$n$排列包含1234模式，$B$事件表示$n$排列包含1324模式。令$X$表示随机选择一个$n$排列包含1234模式，$Y$表示随机选择一个$n$排列包含1324模式。那么$E(X|A),E(Y|B)$哪个大呢？  
**Solution.** 随机选择四个不同的数，组成$q$模式的概率是$1/24$，那么$n$排列任选四个数的个数是$\begin{pmatrix}n\\4\end{pmatrix}$，所以$E(X)=E(Y)=\begin{pmatrix}n\\4\end{pmatrix}/24$。根据**Theorem 15.25**有
$$E(X) = E(X|A)P(A) + E(X|\bar{A})P(\bar{A}) = E(X|A)P(A)$$
$$E(Y) = E(Y|B)P(B) + E(Y|\bar{B})P(\bar{B}) = E(Y|B)P(B)$$
由于$E(X)=E(Y)$，那么$E(X|A)P(A)=E(Y|B)P(B)$，又有$P(A)>P(B)$，所以$E(Y|B)>E(X|A)$。

(18) 令$X(p)$是随机选择的$n$排列$p$的固定点个数。求证$\text{Var}(X)=1$。  
**Solution.** $E(X)=1$，又有$\text{Var}(X)=E(X^2)-E(X)^2$，那么需要证明的就是$E(X^2)=2$。  
和**Theorem 15.21**一样定义指示随机变量$X_i$，那么
$$E(X^2)=E((\sum_{i=1}^n X_i)^2)=\sum_{i=1}^n E(X_i^2)+2\sum_{i<j}E(X_iX_j)$$
$X_i$只能是零和一，那么$X_i=X_i^2$，所以$E(X_i^2)=E(X_i)=1/n$。如果$p$是随机选择的$n$排列，那么对于$i<j$，有$1/n(n-1)$的几率有$p_i=i,p_j=j$，所以$E(X_iX_j)=\frac{1}{n(n-1)}$。所以
$$E(X^2)=n\cdot\frac{1}{n}+\begin{pmatrix}
n\\2
\end{pmatrix}\cdot\frac{1}{n(n-1)}=1+1=2$$
